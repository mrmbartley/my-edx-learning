---
title: "Linear Regression"
author: "Matt Bartley"
date: "7/19/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dslabs)
library(Lahman)
library(HistData)
```

## Background

This is a walkthrough of the HarvardX data science course on linear regression. We start by examining a motivating example based on Moneyball, a movie on the evolution of baseball statistics. Note the following useful definitions:

1. Plate appearance (PA): each time the batter has an opportunity to hit, it is called a plate appearance
2. Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.
3. Single: the batter hits the ball and gets to first base.
4. Double (2B): the batter hits the ball and gets to second base.
5. Triple (3B): the batter hits the ball and gets to third base.
6. Home Run (HR): the batter hits the ball and goes all the way home and scores a run.

Note that a significant portion of the code is taken from the actual course.\
https://courses.edx.org/courses/course-v1:HarvardX+PH125.7x+2T2020/

There a number of hypotheses we can examine through exploratory data analysis and regression analysis. An obvious example, we can look at the relationship between home runs and runs per game. Not surprisingly, we see a corresponding linear relationship.


```{r Bases on Balls or Stolen Bases}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR_per_game = HR / G, R_per_game = R / G) %>%
  ggplot(aes(HR_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```


```{r Baseball Questions}

NT <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(AB_per_game = AB/G, 
         R_per_game = R/G, 
         HR_per_game = HR / G, 
         W_per_game = W/G, 
         E_per_game = E/G,
         X3B_per_game = X3B/G,
         X2B_per_game = X2B/G)

NT  %>% ggplot(aes(AB_per_game, R_per_game)) +
  geom_point(alpha = 0.5)

NT %>% ggplot(aes(E_per_game, W_per_game)) +
  geom_point(alpha = 0.5)

NT %>% ggplot(aes(X2B_per_game, X3B_per_game)) +
  geom_point(alpha = 0.5)


```

## Correlation

A historical data set from Galton where he attempted to predict sons' heights based on fathers' heights will help us understand the concept of correlation. Using typical summary statistics like mean and standard deviation doesn't inform us well on the relationship between the heights of fathers and their sons, which is evident in the scatter plot.


```{r Galton heights data}
data("GaltonFamilies")

set.seed(1983)

galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

# means and standard deviations
galton_heights %>%
    summarize(mean(father), sd(father), mean(son), sd(son))

# scatterplot of father and son heights
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5)
```



Instead we can calculate the correlation coefficient which helps us understand this relationship. Note that we are computing this on a random sample and so repeated sampling would result in different results for our correlation estimate. We might think to use Monte Carlo simulation to assess a possible confidence interval for our estimate of the correlation. First we use a Q-Q plot to establish whether normality is sufficient to build our interval. We can see that it isn't.


```{r correlation coefficient}
galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)

# compute sample correlation
R <- sample_n(galton_heights, 25, replace = TRUE) %>%
    summarize(r = cor(father, son))
R

# Monte Carlo simulation to show distribution of sample correlation
B <- 1000
N <- 25
R <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>%
    summarize(r = cor(father, son)) %>%
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))

# expected value and standard error
mean(R)
sd(R)

# QQ-plot to evaluate whether N is large enough
data.frame(R) %>%
    ggplot(aes(sample = R)) +
    stat_qq() +
    geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))
```



```{r Correlation Questions }

NT <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(AB_per_game = AB/G, 
         R_per_game = R/G, 
         HR_per_game = HR / G, 
         W_per_game = W/G, 
         E_per_game = E/G,
         X3B_per_game = X3B/G,
         X2B_per_game = X2B/G)

NT %>% summarize(r = cor(R_per_game, AB_per_game)) %>% pull(r)

NT %>% summarize(r = cor(W_per_game, E_per_game)) %>% pull(r)

NT %>% summarize(r = cor(X2B_per_game, X3B_per_game)) %>% pull(r)

```

Note that correlation is only useful in a specific context. There are many possible patterns between pairs of data that can present with the same correlation. Anscombe's Quartet is one such example. 

Remember our goal is to predict the son's height based on the father's height. A general prediction would be to take the average height of the sons. If however we want to predict the height of a given son, then we have more information. We are predicting the son's height given a specific height for the father. In this case, the average height of sons is not a good prediction.

We can stratify the height data and look at the relationship between father and son heights for each stratum of father heights. If we further plot the means of the sons heights we will notice that the 'line of best fit' associated with the relationship between stratified father's heights and the conditional average height of their sons is equal to the correlation between sons' and fathers' heights.

> Key idea: for every standard deviation increase above the average, our prediction grows by the correlation times the standard deviation above the predicted average

```{r Regression Line}
# number of fathers with height 72 or 72.5 inches
sum(galton_heights$father == 72)
sum(galton_heights$father == 72.5)

# predicted height of a son with a 72 inch tall father
conditional_avg <- galton_heights %>%
    filter(round(father) == 72) %>%
    summarize(avg = mean(son)) %>%
    pull(avg)
conditional_avg

# stratify fathers' heights to make a boxplot of son heights
galton_heights %>% mutate(father_strata = factor(round(father))) %>%
    ggplot(aes(father_strata, son)) +
    geom_boxplot() +
    geom_point()

# center of each boxplot
galton_heights %>%
    mutate(father = round(father)) %>%
    group_by(father) %>%
    summarize(son_conditional_avg = mean(son)) %>%
    ggplot(aes(father, son_conditional_avg)) +
    geom_point()

# calculate values to plot regression line on original data
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x

# add regression line to plot
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = b, slope = m)

```


Correlation is often motivated by the bivariate normal distribution which is defined for pairs of random variables. If we believe the pairs of random variables are independently normal, then the conditional distribution of one variable given the other is also approximately normal as well. Hence if we have father's heights and son's heights both being normal, and conditionally on the father's heights, the son's heights are approximately normal, then the bivariate distribution of father and son heights is also normal.

To validate this assumption we assess whether the son's heights are approximately normal conditioning on the stratum of the father's height. Furthermore, if we validate that the data is approximately bivariate then the conditional expectations is given by the regression line.


```{r Bivariate Normal}
galton_heights %>%
  mutate(z_father = round((father - mean(father)) / sd(father))) %>%
  filter(z_father %in% -2:2) %>%
  ggplot() +  
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)

# compute a regression line to predict the son's height from the father's height
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x

# compute a regression line to predict the father's height from the son's height
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y

```



```{r random}
0.5*3/2

```


```{r Correlation Question 2}
set.seed(1989, sample.kind="Rounding") #if you are using R 3.6 or later

female_heights <- GaltonFamilies%>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight)


stats <- female_heights %>% summarize_each(funs(mean,sd))
corr <- female_heights %>% summarize(r = cor(mother,daughter)) %>% pull(r)


# slope daughter from mother
slope <- corr * stats$daughter_sd / stats$mother_sd
slope
stats$daughter_mean - stats$mother_mean * corr * stats$daughter_sd / stats$mother_sd 
corr**2
stats$daughter_mean + slope*(60 - stats$mother_mean)

```

## Linear Models Intro

The regression for predicting runs from BB versus singles would suggest that BB is more predictive. However, this does not account for the confounding effect between HR and BB due to the fact that HR hitters are often walked. We demonstrate this via the positive correlation between HR and BB.

```{r BB and predicting runs}
# find regression line for predicting runs from BBs
library(tidyverse)
library(Lahman)
bb_slope <- Teams %>% 
  filter(yearID %in% 1961:2001 ) %>% 
  mutate(BB_per_game = BB/G, R_per_game = R/G) %>% 
  lm(R_per_game ~ BB_per_game, data = .) %>% 
  .$coef %>%
  .[2]
bb_slope

# compute regression line for predicting runs from singles
singles_slope <- Teams %>% 
  filter(yearID %in% 1961:2001 ) %>%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %>%
  lm(R_per_game ~ Singles_per_game, data = .) %>%
  .$coef  %>%
  .[2]
singles_slope

# calculate correlation between HR, BB and singles
Teams %>% 
  filter(yearID %in% 1961:2001 ) %>% 
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %>%  
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB,Singles))
```

We can investigate this hypothesis further by examining the regression results after stratifying HRs. Once we account for these different levels, we can see that BB is no more predictive of runs than singles, which is consistent with what we would expect. Similarly, when stratifying by BB we see that HRs have a relatively consistent slope across regression lines.

Hence by holding HRs constant, the relationship between runs and BB is linear, and the slope of this relationship does not depend on the number of home runs.


```{r Stratification and MV Regression}

# stratify HR per game to nearest 10, filter out strata with few points
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR_strata = round(HR/G, 1), 
         BB_per_game = BB / G,
         R_per_game = R / G) %>%
  filter(HR_strata >= 0.4 & HR_strata <=1.2)
  
# scatterplot for each HR stratum
dat %>% 
  ggplot(aes(BB_per_game, R_per_game)) +  
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ HR_strata)
  
# calculate slope of regression line after stratifying by HR
dat %>%  
  group_by(HR_strata) %>%
  summarize(slope = cor(BB_per_game, R_per_game)*sd(R_per_game)/sd(BB_per_game))
  
# stratify by BB
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(BB_strata = round(BB/G, 1), 
         HR_per_game = HR / G,
         R_per_game = R / G) %>%
  filter(BB_strata >= 2.8 & BB_strata <=3.9) 

# scatterplot for each BB stratum
dat %>% ggplot(aes(HR_per_game, R_per_game)) +  
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ BB_strata)
  
# slope of regression line after stratifying by BB
dat %>%  
  group_by(BB_strata) %>%
  summarize(slope = cor(HR_per_game, R_per_game)*sd(R_per_game)/sd(HR_per_game)) 

```

## Least Squares Estimate

```{r RSS}

# compute RSS for any pair of beta0 and beta1 in Galton's data
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
rss <- function(beta0, beta1, data){
    resid <- galton_heights$son - (beta0+beta1*galton_heights$father)
    return(sum(resid^2))
}

# plot RSS as a function of beta1 when beta0=25
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss))

# fit regression line to predict son's height from father's height
fit <- lm(son ~ father, data = galton_heights)
fit

# summary statistics
summary(fit)

```


```{r}
# Monte Carlo simulation
B <- 1000
N <- 50
lse <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% 
    .$coef 
})
lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 

# Plot the distribution of beta_0 and beta_1
library(gridExtra)
p1 <- lse %>% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = "black") 
p2 <- lse %>% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = "black") 
grid.arrange(p1, p2, ncol = 2)

# summary statistics
sample_n(galton_heights, N, replace = TRUE) %>% 
  lm(son ~ father, data = .) %>% 
  summary %>%
  .$coef
  
lse %>% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))

```


```{r advanced LSE}

lse %>% summarize(cor(beta_0, beta_1))

B <- 1000
N <- 50
lse <- replicate(B, {
      sample_n(galton_heights, N, replace = TRUE) %>%
      mutate(father = father - mean(father)) %>%
      lm(son ~ father, data = .) %>% .$coef 
})

cor(lse[1,], lse[2,])

```


```{r prediction}
# plot predictions and confidence intervals
galton_heights %>% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = "lm")
  
# predict Y directly
fit <- galton_heights %>% lm(son ~ father, data = .) 
Y_hat <- predict(fit, se.fit = TRUE)
names(Y_hat)

# plot best fit line
galton_heights %>%
  mutate(Y_hat = predict(lm(son ~ father, data=.))) %>%
  ggplot(aes(father, Y_hat))+
  geom_line()

```



```{r LSE Questions}

# In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for  β1  if we assume  β^0  is 36?

beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 36))
results
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)


#Load the Lahman library and filter the Teams data frame to the years 1961-2001. Run a linear model in R predicting the number of runs per game based on both the number of bases on balls per game and the number of home runs per game.

# What is the coefficient for bases on balls?
  
NT <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(RpG = R/G, BBpG = BB/G, HRpG = HR/G)

lm(RpG ~ BBpG + HRpG, NT)

#We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute the regression slope coefficients for each sample:

# What does the central limit theorem tell us about the variables beta_0 and beta_1?
# - They are approximately normally distributed.(CORRECT)
# - The expected value of each is the true value of  β0  and  β1 (assuming the Galton heights data is a complete population). (CORRECT)
# - The central limit theorem does not apply in this situation. (WRONG)
# - It allows us to test the hypothesis that  β0=0  and  β1=0 . (WRONG)

B <- 1000
N <- 100
lse <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% .$coef 
})

lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 
lse

p1 <- lse %>% ggplot(aes(beta_0)) + geom_histogram(binwidth = 5, color = "black") 
p2 <- lse %>% ggplot(aes(beta_1)) + geom_histogram(binwidth = 0.1, color = "black") 
grid.arrange(p1, p2, ncol = 2)


# Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights?

model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model, interval = c("confidence"), level = 0.95)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
    geom_line(color = "blue", size = 1) + 
    geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + 
    geom_point(data = galton_heights, aes(x = father, y = son))
        


galton_heights %>% ggplot(aes(father, son)) +
    geom_point() +
    geom_smooth(method = "lm")
        
```

```{r}

# Define female_heights, a set of mother and daughter heights sampled from GaltonFamilies, as follows:

set.seed(1989, sample.kind="Rounding") #if you are using R 3.6 or later
library(HistData)
data("GaltonFamilies")
options(digits = 3)    # report 3 significant digits

female_heights <- GaltonFamilies %>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight)


model <- lm(mother ~ daughter, female_heights)
model
data.frame(predicted = predict(model), actual = female_heights$mother)

```

```{r}
library(Lahman)

bat_02 <- Batting %>% filter(yearID == 2002) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    filter(pa >= 100) %>%
    select(playerID, singles, bb)

# How many players had a single rate mean_singles of greater than 0.2 per plate appearance over 1999-2001?

bat <- Batting %>% filter(yearID %in% 1999:2001) %>%
    group_by(yearID) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    ungroup() %>%
    filter(pa >= 100) %>%
    select(playerID, yearID, singles, bb) %>%
    group_by(playerID) %>%
    summarize(mean_singles = mean(singles), mean_bb = mean(bb))

sum(bat$mean_singles > 0.2)
sum(bat$mean_bb > 0.2)

# What is the correlation between 2002 singles rates and 1999-2001 average singles rates?

new_bat <- inner_join(bat, bat_02)
new_bat %>% summarize(r = cor(singles, mean_singles))
new_bat %>% summarize(r = cor(bb, mean_bb))


# Make scatterplots of mean_singles versus singles and mean_bb versus bb. Are either of these distributions bivariate normal?

p1 <- new_bat %>% ggplot(aes(singles, mean_singles)) + geom_point()
p2 <- new_bat %>% ggplot(aes(bb, mean_bb)) + geom_point()
grid.arrange(p1, p2, ncol = 2)

#  Fit a linear model to predict 2002 singles given 1999-2001 mean_singles. What is the coefficient of mean_singles, the slope of the fit?

lm(singles ~ mean_singles, new_bat)

# Fit a linear model to predict 2002 bb given 1999-2001 mean_bb. What is the coefficient of mean_bb, the slope of the fit?
  
lm(bb ~ mean_bb, new_bat)

```





